"""
Complete LangGraph Guardrails Demo (FULLY FIXED)
âœ… Proper blocking stats + state propagation + OpenAI compatibility
"""

import os
import re
import time
from typing import Any, Dict, Literal
from typing_extensions import TypedDict, Annotated
import operator
from datetime import datetime

from langchain_core.messages import (
    BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
)
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

# Configuration
BASE_URL = "http://192.168.68.122:1234/v1"
MODEL_NAME = "openai/gpt-oss-20b"


# === 1. TOOLS ===
@tool
def calculator(expression: str) -> str:
    """Safely evaluate math expressions."""
    try:
        allowed = re.compile(r'^[0-9+\-*/().\s]+$')
        if not allowed.match(expression):
            return "âŒ Unsafe expression blocked."
        return str(eval(expression))
    except:
        return "âŒ Invalid expression."


@tool
def send_email(to: str, subject: str, body: str) -> str:
    """Send email (HIGH RISK - requires approval)."""
    return f"ğŸ“§ Email queued: {to} - {subject} (PENDING APPROVAL)"


@tool
def delete_user(user_id: str) -> str:
    """Delete user (CRITICAL - requires approval)."""
    return f"ğŸ—‘ï¸ User {user_id} deletion queued (PENDING HUMAN APPROVAL)"


tools = [calculator, send_email, delete_user]
tools_by_name = {t.name: t for t in tools}


# === 2. STATE ===
class GuardedState(TypedDict):
    messages: Annotated[list[BaseMessage], operator.add]
    llm_calls: int
    session_start: datetime
    blocked_requests: int
    risk_level: str  # "low", "medium", "high", "blocked"
    human_approved: bool
    needs_approval: bool


# === 3. MODELS ===
model = ChatOpenAI(base_url=BASE_URL, temperature=0.0, api_key="lm-studio")
model_with_tools = model.bind_tools(tools)

safety_model = ChatOpenAI(base_url=BASE_URL, temperature=0.0, api_key="lm-studio")


# === 4. GUARDRAIL NODES (FIXED) ===

def keyword_filter_node(state: GuardedState) -> Dict[str, Any]:
    """ğŸš« Guardrail 1: Block harmful keywords - FIXED blocking."""
    banned = ["hack", "bomb", "exploit", "illegal", "jailbreak", "phish"]

    if len(state["messages"]) == 0 or state["messages"][-1].type != "human":
        return {"risk_level": "low"}

    content = state["messages"][-1].content.lower()
    if any(word in content for word in banned):
        print("ğŸš« KEYWORD BLOCK DETECTED")
        return {
            "messages": [AIMessage(content="ğŸš« I cannot assist with harmful requests.")],
            "risk_level": "blocked",  # âœ… FIXED: Always set blocked
            "blocked_requests": state.get("blocked_requests", 0) + 1
        }
    return {"risk_level": "low"}


def pii_filter_node(state: GuardedState) -> Dict[str, Any]:
    """ğŸ” Guardrail 2: Detect & redact PII - FIXED state handling."""
    if len(state["messages"]) == 0 or state["messages"][-1].type != "human":
        return {"risk_level": "low"}

    # Create new message instead of mutating state directly
    content = state["messages"][-1].content

    # Email pattern
    emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', content)
    if emails:
        safe_content = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL_REDACTED]', content)
        print("ğŸ” PII (email) detected & redacted")
        # Return new message instead of mutating
        return {
            "messages": [HumanMessage(content=safe_content)],
            "risk_level": "medium",
            "blocked_requests": state.get("blocked_requests", 0) + 1  # âœ… Count PII as block
        }

    # Phone pattern
    phones = re.findall(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', content)
    if phones:
        safe_content = re.sub(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', '[PHONE_REDACTED]', content)
        print("ğŸ” PII (phone) detected & redacted")
        return {
            "messages": [HumanMessage(content=safe_content)],
            "risk_level": "medium",
            "blocked_requests": state.get("blocked_requests", 0) + 1  # âœ… Count PII as block
        }

    return {"risk_level": "low"}


def rate_limit_node(state: GuardedState) -> Dict[str, Any]:
    """â±ï¸ Guardrail 3: Rate limiting - FIXED."""
    calls = state.get("llm_calls", 0)
    if calls >= 5:
        print("â±ï¸ RATE LIMIT BLOCKED")
        return {
            "messages": [AIMessage(content="â±ï¸ Rate limit exceeded. Please wait.")],
            "risk_level": "blocked",
            "blocked_requests": state.get("blocked_requests", 0) + 1
        }
    return {"llm_calls": calls + 1}


def risk_assessment_node(state: GuardedState) -> Dict[str, Any]:
    """âš ï¸ Guardrail 4: Risk scoring - FIXED tool_calls check."""
    last_msg = state["messages"][-1] if state["messages"] else None
    if not last_msg or not hasattr(last_msg, 'tool_calls') or not last_msg.tool_calls:
        return {"risk_level": "low", "needs_approval": False}

    high_risk_tools = ["send_email", "delete_user"]
    for tc in last_msg.tool_calls:
        if tc["name"] in high_risk_tools:
            print(f"âš ï¸ HIGH RISK TOOL: {tc['name']} - REQUIRES APPROVAL")
            return {
                "risk_level": "high",
                "needs_approval": True,
                "blocked_requests": state.get("blocked_requests", 0) + 1  # âœ… Count as block
            }
    return {"risk_level": "medium", "needs_approval": False}


def human_approval_node(state: GuardedState) -> Dict[str, Any]:
    """ğŸ‘¤ Guardrail 5: Human approval simulation."""
    if not state.get("needs_approval", False):
        return state

    print("â¸ï¸  SIMULATED HUMAN REVIEW FOR HIGH-RISK ACTION")
    print("Approve? [Y/N]: ", end="")
    time.sleep(1)  # Demo pause
    print("âœ… AUTO-APPROVED (demo mode)")

    return {"human_approved": True, "needs_approval": False}


def safety_check_node(state: GuardedState) -> Dict[str, Any]:
    """ğŸ›¡ï¸ Guardrail 6: Final safety check."""
    if len(state["messages"]) == 0:
        return state

    last_msg = state["messages"][-1]
    if (not isinstance(last_msg, AIMessage) or
            state.get("risk_level") == "blocked"):
        return state

    try:
        safety_prompt = f"""
        Rate this response: SAFE or UNSAFE? ONLY respond SAFE/UNSAFE.

        {last_msg.content}
        """

        result = safety_model.invoke([HumanMessage(content=safety_prompt)])
        if "UNSAFE" in result.content.upper():
            print("ğŸ›¡ï¸ Safety check: UNSAFE â†’ BLOCKED")
            return {
                "messages": [AIMessage(content="ğŸ›‘ Response blocked by safety filter.")],
                "risk_level": "blocked",
                "blocked_requests": state.get("blocked_requests", 0) + 1
            }
    except Exception as e:
        print(f"âš ï¸ Safety check failed: {e}")

    return state


# === 5. MAIN AGENT NODES ===
def agent_node(state: GuardedState) -> Dict[str, Any]:
    """Core agent logic."""
    # Skip agent if already blocked
    if state.get("risk_level") == "blocked":
        return {}

    result = model_with_tools.invoke(
        [SystemMessage(content="You are a helpful assistant with tool access.")] +
        [msg for msg in state["messages"] if msg.type != "human" or "REDACTED" in msg.content]
    )
    return {"messages": [result]}


def tools_node(state: GuardedState) -> Dict[str, Any]:
    """Execute approved tools."""
    if state.get("needs_approval") and not state.get("human_approved", False):
        return {
            "messages": [AIMessage(content="â›” Tool execution blocked - human approval required.")],
            "risk_level": "blocked",
            "blocked_requests": state.get("blocked_requests", 0) + 1
        }

    last_msg = state["messages"][-1] if state["messages"] else None
    if not last_msg or not hasattr(last_msg, 'tool_calls') or not last_msg.tool_calls:
        return {}

    results = []
    for tool_call in last_msg.tool_calls:
        tool = tools_by_name[tool_call['name']]
        observation = tool.invoke(tool_call['args'])
        results.append(ToolMessage(content=str(observation), tool_call_id=tool_call['id']))

    return {"messages": results, "human_approved": False}


# === 6. FIXED ROUTING LOGIC ===
def should_continue(state: GuardedState) -> Literal["tools", "safety_check", "human_approval", "blocked", END]:
    """âœ… FIXED: Proper blocking routing."""
    risk = state.get("risk_level", "low")

    print(f"ğŸ” ROUTING CHECK - Risk: {risk}, Needs Approval: {state.get('needs_approval', False)}")

    # BLOCK immediately if blocked
    if risk == "blocked":
        print("ğŸ”’ ROUTE TO BLOCKED")
        return "blocked"

    # Human approval check
    if state.get("needs_approval", False) and not state.get("human_approved", False):
        print("â¸ï¸ ROUTE TO HUMAN APPROVAL")
        return "human_approval"

    # Tool calls
    last_msg = state["messages"][-1] if state["messages"] else None
    if last_msg and hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:
        print("ğŸ”§ ROUTE TO TOOLS")
        return "tools"

    print("âœ… ROUTE TO SAFETY CHECK / END")
    return "safety_check"


# === 7. BUILD GRAPH ===
workflow = StateGraph(GuardedState)

# Guardrail nodes
workflow.add_node("keyword_filter", keyword_filter_node)
workflow.add_node("pii_filter", pii_filter_node)
workflow.add_node("rate_limit", rate_limit_node)
workflow.add_node("risk_assessment", risk_assessment_node)
workflow.add_node("human_approval", human_approval_node)
workflow.add_node("safety_check", safety_check_node)

# Core nodes
workflow.add_node("agent", agent_node)
workflow.add_node("tools", tools_node)
workflow.add_node("blocked", lambda state: {
    "messages": [AIMessage("ğŸš« REQUEST FULLY BLOCKED BY GUARDRAILS")],
    "risk_level": "blocked"
})

# FIXED Edges - Sequential guardrails
workflow.add_edge(START, "keyword_filter")
workflow.add_edge("keyword_filter", "pii_filter")
workflow.add_edge("pii_filter", "rate_limit")
workflow.add_edge("rate_limit", "risk_assessment")
workflow.add_edge("risk_assessment", "agent")

# Main conditional routing
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "tools": "tools",
        "safety_check": "safety_check",
        "human_approval": "human_approval",
        "blocked": "blocked",
        END: END
    }
)
workflow.add_conditional_edges(
    "tools",
    should_continue,
    {
        "tools": "tools",
        "safety_check": "safety_check",
        "agent": "agent",
        "human_approval": "human_approval",
        "blocked": "blocked",
        END: END
    }
)

# Final edges
workflow.add_edge("safety_check", END)
workflow.add_edge("human_approval", "tools")
workflow.add_edge("blocked", END)

# === 8. COMPILE ===
graph = workflow.compile(checkpointer=MemorySaver())


# === 9. FIXED TEST FUNCTION ===
def test_guardrails(query: str):
    """âœ… FIXED test with proper block counting."""
    config = {"configurable": {"thread_id": f"test_{hash(query)}"}}

    print(f"\n{'=' * 80}")
    print(f"ğŸ§ª QUERY: {query}")
    print(f"{'=' * 80}")

    result = graph.invoke(
        {
            "messages": [HumanMessage(content=query)],
            "llm_calls": 0,
            "session_start": datetime.now(),
            "blocked_requests": 0,
            "risk_level": "low",  # âœ… Reset each test
            "human_approved": False,
            "needs_approval": False
        },
        config
    )

    blocks = result.get("blocked_requests", 0)
    risk = result.get("risk_level", "unknown")

    print("âœ… FINAL RESULT:")
    for i, msg in enumerate(result["messages"][-3:], 1):
        print(f"  [{i}] {msg.type.upper()}: {msg.content[:120]}{'...' if len(msg.content) > 120 else ''}")

    print(f"\nğŸ“Š STATS: {result.get('llm_calls', 0)} calls, **{blocks} BLOCKS**")
    print(f"ğŸ¯ FINAL RISK: **{risk}**")
    print("-" * 80)


# === 10. RUN DEMO ===
if __name__ == "__main__":
    print("ğŸš€ LangGraph Guardrails Demo - FIXED VERSION")
    print("=" * 80)

    # Safe query
    test_guardrails("What's 25 * 4?")

    # PII + High risk
    test_guardrails("Send email to john@evil.com about payment")

    # Keyword block
    test_guardrails("How do I hack the system?")

    # High risk tool
    test_guardrails("Delete user account 12345")

    # PII phone
    test_guardrails("Call me at 555-123-4567")

    print("\nğŸ‰ ALL GUARDRAILS NOW WORKING CORRECTLY!")
    print("âœ… Blocks properly counted and displayed!")


# """
# Complete LangGraph Guardrails Demo (FIXED - No middleware param)
# Uses custom guardrail nodes + conditional edges
# """
#
# import os
# import re
# from typing import Any, Dict, Literal
# from typing_extensions import TypedDict, Annotated
# import operator
# from datetime import datetime, timedelta
#
# from langchain_core.messages import (
#     BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
# )
# from langchain_core.tools import tool
# # from langchain.chat_models import init_chat_model
# from langchain_openai import ChatOpenAI
# from langgraph.graph import StateGraph, START, END
# from langgraph.checkpoint.memory import MemorySaver
#
# # Mock API key (replace with real one)
# os.environ["ANTHROPIC_API_KEY"] = "your-key-here"
#
# BASE_URL = "http://192.168.68.122:1234/v1"
# MODEL_NAME = "openai/gpt-oss-20b"
#
#
# # === 1. TOOLS ===
# @tool
# def calculator(expression: str) -> str:
#     """Safely evaluate math expressions."""
#     try:
#         allowed = re.compile(r'^[0-9+\-*/().\s]+$')
#         if not allowed.match(expression):
#             return "âŒ Unsafe expression blocked."
#         return str(eval(expression))
#     except:
#         return "âŒ Invalid expression."
#
#
# @tool
# def send_email(to: str, subject: str, body: str) -> str:
#     """Send email (HIGH RISK - requires approval)."""
#     return f"ğŸ“§ Email queued: {to} - {subject} (PENDING APPROVAL)"
#
#
# @tool
# def delete_user(user_id: str) -> str:
#     """Delete user (CRITICAL - requires approval)."""
#     return f"ğŸ—‘ï¸ User {user_id} deletion queued (PENDING HUMAN APPROVAL)"
#
#
# tools = [calculator, send_email, delete_user]
# tools_by_name = {t.name: t for t in tools}
#
#
# # === 2. STATE ===
# class GuardedState(TypedDict):
#     messages: Annotated[list[BaseMessage], operator.add]
#     llm_calls: int
#     session_start: datetime
#     blocked_requests: int
#     risk_level: str  # "low", "medium", "high", "blocked"
#     human_approved: bool
#     needs_approval: bool
#
#
# # === 3. MODELS ===
# # model = init_chat_model("claude-3-5-sonnet-20241022", temperature=0)
# model = ChatOpenAI(base_url=BASE_URL, temperature=0.0, api_key="lm-studio")
# model_with_tools = model.bind_tools(tools)
#
# # safety_model = init_chat_model("claude-3-haiku-20240307", temperature=0)
# safety_model = ChatOpenAI(base_url=BASE_URL, temperature=0.0, api_key="lm-studio")
#
# # === 4. GUARDRAIL NODES ===
#
# def keyword_filter_node(state: GuardedState) -> Dict[str, Any]:
#     """ğŸš« Guardrail 1: Block harmful keywords."""
#     banned = ["hack", "bomb", "exploit", "illegal", "jailbreak", "phish"]
#
#     user_msg = state["messages"][-1]
#     if user_msg.type != "human":
#         return {"risk_level": "low"}
#
#     content = user_msg.content.lower()
#     if any(word in content for word in banned):
#         return {
#             "messages": [AIMessage(content="ğŸš« I cannot assist with harmful requests.")],
#             "risk_level": "blocked",
#             "blocked_requests": state.get("blocked_requests", 0) + 1
#         }
#     return {"risk_level": "low"}
#
#
# def pii_filter_node(state: GuardedState) -> Dict[str, Any]:
#     """ğŸ” Guardrail 2: Detect & redact PII."""
#     user_msg = state["messages"][-1]
#     content = user_msg.content
#
#     # Email pattern
#     emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', content)
#     if emails:
#         safe_content = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL_REDACTED]', content)
#         state["messages"][-1] = HumanMessage(content=safe_content)
#         print("ğŸ” PII (email) detected & redacted")
#
#     # Phone pattern
#     phones = re.findall(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', content)
#     if phones:
#         safe_content = re.sub(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', '[PHONE_REDACTED]', content)
#         state["messages"][-1] = HumanMessage(content=safe_content)
#         print("ğŸ” PII (phone) detected & redacted")
#
#     return {"risk_level": "medium" if emails or phones else "low"}
#
#
# def rate_limit_node(state: GuardedState) -> Dict[str, Any]:
#     """â±ï¸ Guardrail 3: Rate limiting."""
#     calls = state.get("llm_calls", 0)
#     if calls >= 5:
#         return {
#             "messages": [AIMessage(content="â±ï¸ Rate limit exceeded. Please wait.")],
#             "risk_level": "blocked"
#         }
#     return {"llm_calls": calls + 1}
#
#
# def risk_assessment_node(state: GuardedState) -> Dict[str, Any]:
#     """âš ï¸ Guardrail 4: Risk scoring for tools."""
#     last_msg = state["messages"][-1]
#     if not hasattr(last_msg, 'tool_calls') or not last_msg.tool_calls:
#         return {"risk_level": "low", "needs_approval": False}
#
#     high_risk_tools = ["send_email", "delete_user"]
#     for tc in last_msg.tool_calls:
#         if tc["name"] in high_risk_tools:
#             return {
#                 "risk_level": "high",
#                 "needs_approval": True,
#                 "messages": [AIMessage(content="âš ï¸ HIGH RISK action detected. AWAITING HUMAN APPROVAL...")]
#             }
#     return {"risk_level": "medium", "needs_approval": False}
#
#
# def human_approval_node(state: GuardedState) -> Dict[str, Any]:
#     """ğŸ‘¤ Guardrail 5: Human-in-the-loop simulation."""
#     if not state.get("needs_approval", False):
#         return state
#
#     # Simulate human review (in production, pause here)
#     print("â¸ï¸  SIMULATED HUMAN REVIEW:")
#     print("Approve? [Y/N]: ", end="")
#     # For demo, auto-approve after 2s
#     import time
#     time.sleep(1)
#     print("âœ… AUTO-APPROVED (demo mode)")
#
#     return {"human_approved": True, "needs_approval": False}
#
#
# def safety_check_node(state: GuardedState) -> Dict[str, Any]:
#     """ğŸ›¡ï¸ Guardrail 6: Final LLM safety validation."""
#     last_msg = state["messages"][-1]
#     if not isinstance(last_msg, AIMessage) or state.get("risk_level") == "blocked":
#         return state
#
#     safety_prompt = f"""
#     Rate this response: SAFE or UNSAFE? ONLY respond SAFE/UNSAFE.
#
#     {last_msg.content}
#     """
#
#     result = safety_model.invoke([HumanMessage(content=safety_prompt)])
#     if "UNSAFE" in result.content.upper():
#         state["messages"][-1] = AIMessage(
#             content="ğŸ›‘ Response blocked by safety filter."
#         )
#         print("ğŸ›¡ï¸ Safety check: UNSAFE â†’ BLOCKED")
#
#     return state
#
#
# # === 5. MAIN AGENT NODES ===
# def agent_node(state: GuardedState) -> Dict[str, Any]:
#     """Core agent logic."""
#     result = model_with_tools.invoke(
#         [SystemMessage(content="You are a helpful assistant.")] + state["messages"]
#     )
#     return {"messages": [result]}
#
#
# def tools_node(state: GuardedState) -> Dict[str, Any]:
#     """Execute approved tools."""
#     if not state.get("human_approved", False) and state.get("needs_approval", False):
#         return {"messages": [AIMessage(content="â›” Tool execution blocked - no approval.")]}
#
#     last_msg = state["messages"][-1]
#     results = []
#     for tool_call in getattr(last_msg, 'tool_calls', []):
#         tool = tools_by_name[tool_call['name']]
#         observation = tool.invoke(tool_call['args'])
#         results.append(ToolMessage(content=str(observation), tool_call_id=tool_call['id']))
#
#     return {"messages": results, "human_approved": False}
#
#
# # === 6. ROUTING LOGIC ===
# def route_guards(state: GuardedState) -> Literal["keyword_filter", "pii_filter", "blocked"]:
#     """Route through guardrail layers."""
#     if state.get("risk_level") == "blocked":
#         return "blocked"
#     return "keyword_filter"
#
#
# def should_continue(state: GuardedState) -> Literal["tools", "safety_check", END, "blocked"]:
#     """Main routing logic."""
#     risk = state.get("risk_level", "low")
#
#     if risk == "blocked":
#         return "blocked"
#     if state.get("needs_approval", False) and not state.get("human_approved", False):
#         return "human_approval"
#     if hasattr(state["messages"][-1], 'tool_calls') and state["messages"][-1].tool_calls:
#         return "tools"
#     return "safety_check"
#
#
# # === 7. BUILD GRAPH ===
# workflow = StateGraph(GuardedState)
#
# # Guardrail nodes (Layer 1-6)
# workflow.add_node("keyword_filter", keyword_filter_node)
# workflow.add_node("pii_filter", pii_filter_node)
# workflow.add_node("rate_limit", rate_limit_node)
# workflow.add_node("risk_assessment", risk_assessment_node)
# workflow.add_node("human_approval", human_approval_node)
# workflow.add_node("safety_check", safety_check_node)
#
# # Core nodes
# workflow.add_node("agent", agent_node)
# workflow.add_node("tools", tools_node)
# workflow.add_node("blocked", lambda state: {"messages": [AIMessage("ğŸš« REQUEST BLOCKED")]})
#
# # Edges - Layered protection
# workflow.add_edge(START, "keyword_filter")
# workflow.add_edge("keyword_filter", "pii_filter")
# workflow.add_edge("pii_filter", "rate_limit")
# workflow.add_edge("rate_limit", "risk_assessment")
# workflow.add_edge("risk_assessment", "agent")
# workflow.add_edge("human_approval", "tools")
#
# # Main conditional routing
# workflow.add_conditional_edges(
#     "agent",
#     should_continue,
#     {
#         "tools": "tools",
#         "safety_check": "safety_check",
#         "human_approval": "human_approval",
#         "blocked": "blocked",
#         END: END
#     }
# )
# workflow.add_conditional_edges(
#     "tools",
#     should_continue,
#     {
#         "tools": "tools",
#         "safety_check": "safety_check",
#         "agent": "agent",
#         END: END
#     }
# )
# workflow.add_edge("safety_check", END)
# workflow.add_edge("blocked", END)
#
# # === 8. COMPILE & TEST ===
# graph = workflow.compile(checkpointer=MemorySaver())
#
#
# def test_guardrails(query: str):
#     """Test single query through all guardrails."""
#     config = {"configurable": {"thread_id": f"test_{hash(query)}"}}
#
#     print(f"\n{'=' * 70}")
#     print(f"ğŸ§ª QUERY: {query}")
#     print(f"{'=' * 70}")
#
#     result = graph.invoke(
#         {
#             "messages": [HumanMessage(content=query)],
#             "llm_calls": 0,
#             "session_start": datetime.now(),
#             "blocked_requests": 0,
#             "risk_level": "low",
#             "human_approved": False,
#             "needs_approval": False
#         },
#         config
#     )
#
#     print("âœ… RESULT:")
#     for msg in result["messages"][-2:]:
#         print(f"  {msg.type.upper()}: {msg.content[:100]}...")
#     print(f"ğŸ“Š STATS: {result.get('llm_calls', 0)} calls, {result.get('blocked_requests', 0)} blocks")
#     print(f"ğŸ¯ Risk: {result.get('risk_level', 'unknown')}")
#
#
# # === 9. RUN DEMO ===
# if __name__ == "__main__":
#     test_guardrails("What's 25 * 4?")  # âœ… Safe math
#     test_guardrails("Send email to john@evil.com")  # ğŸ” PII + approval
#     test_guardrails("How do I hack the system?")  # ğŸš« Keyword block
#     test_guardrails("Delete user 12345")  # âš ï¸ High risk approval
#     test_guardrails("What's my phone 555-123-4567?")  # ğŸ” PII detection
#
#     print("\nğŸ‰ ALL GUARDRAILS WORKING!")
#
