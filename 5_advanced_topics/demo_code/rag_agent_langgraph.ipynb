{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27729e38-3b6a-4838-b147-ef6505383031",
   "metadata": {},
   "source": [
    "## Langgraph based Agent-RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9598bf3-040e-4e42-b30c-d8dcb079e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_md import get_retriever\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from get_llm import get_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa12641-dcb7-42f8-b7b8-c531d22e2eff",
   "metadata": {},
   "source": [
    "## Get the retriever: Assume we have already run the ingest and populated the vector store\n",
    "### Check the retriever using some sample queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0bf5226-cd6c-4c3e-8b91-2b171214f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = get_retriever().as_retriever(search_kwargs={\"k\": 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c7ca016-4d63-475e-b294-4d90319fe59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"Explain WebBaseRetriever to load web content\"\n",
    "docs = retriever.invoke(query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b183f5ee-e027-48bf-b21a-98f07e9454a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_retrieved_docs(docs):\n",
    "    for doc in docs:\n",
    "        print(\"-\" * 100)\n",
    "        print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e91193da-cc81-4850-a3f8-20cbfc021a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "The Webbrowser Tool gives your agent the ability to visit a website and extract information. It is described to the agent as\n",
      "\n",
      "```\n",
      "useful for when you need to find something on or summarize a webpage. input should be a comma separated list of \"valid URL including protocol\",\"what you want to find on the page or empty string for a summary\".\n",
      "```\n",
      "\n",
      "It exposes two modes of operation:\n",
      "\n",
      "- when called by the Agent with only a URL it produces a summary of the website contents\n",
      "- when called by the Agent with a URL and a description of what to find it will instead use an in-memory Vector Store to find the most relevant snippets and summarise those\n",
      "\n",
      "## Setup\n",
      "\n",
      "To use the Webbrowser Tool you need to install the dependencies:\n",
      "\n",
      "```bash npm\n",
      "npm install cheerio axios\n",
      "```\n",
      "## Usage, standalone\n",
      "\n",
      "```typescript\n",
      "import { WebBrowser } from \"@langchain/classic/tools/webbrowser\";\n",
      "import { ChatOpenAI, OpenAIEmbeddings } from \"@langchain/openai\";\n",
      "\n",
      "export async function run() {\n",
      "  const model = new ChatOpenAI({ model: \"gpt-4o-mini\", temperature: 0 });\n",
      "  const embeddings = new OpenAIEmbeddings();\n",
      "\n",
      "  const browser = new WebBrowser({ model, embeddings });\n",
      "\n",
      "  const result = await browser.invoke(\n",
      "    `\"https://www.themarginalian.org/2015/04/09/find-your-bliss-joseph-campbell-power-of-myth\",\"who is joseph campbell\"`\n",
      "  );\n",
      "\n",
      "  console.log(result);\n",
      "  /*\n",
      "  Joseph Campbell was a mythologist and writer who discussed spirituality, psychological archetypes, cultural myths, and the mythology of self. He sat down with Bill Moyers for a lengthy conversation at George Lucas’s Skywalker Ranch in California, which continued the following year at the American Museum of Natural History in New York. The resulting 24 hours of raw footage were edited down to six one-hour episodes and broadcast on PBS in 1988, shortly after Campbell’s death, in what became one of the most popular in the history of public television.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "These loaders are used to load web resources. They do not involve the local file system.\n",
      "\n",
      "## All web loaders\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n",
      "\n",
      "If you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n",
      "\n",
      "## Overview\n",
      "\n",
      "### Integration details\n",
      "\n",
      "- TODO: Fill in table features.\n",
      "- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\n",
      "- TODO: Make sure API reference links are correct.\n",
      "\n",
      "| Class | Package | Local | Serializable | JS support|\n",
      "| :--- | :--- | :---: | :---: |  :---: |\n",
      "| [WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html) | [langchain-community](https://python.langchain.com/api_reference/community/index.html) | ✅ | ❌ | ❌ |\n",
      "\n",
      "### Loader features\n",
      "\n",
      "| Source | Document Lazy Loading | Native Async Support\n",
      "| :---: | :---: | :---: |\n",
      "| WebBaseLoader | ✅ | ✅ |\n",
      "\n",
      "## Setup\n",
      "\n",
      "### Credentials\n",
      "\n",
      "`WebBaseLoader` does not require any credentials.\n",
      "\n",
      "### Installation\n",
      "\n",
      "To use the `WebBaseLoader` you first need to install the `langchain-community` python package.\n",
      "\n",
      "```python\n",
      "pip install -qU langchain-community beautifulsoup4\n",
      "```\n",
      "\n",
      "## Initialization\n",
      "\n",
      "Now we can instantiate our model object and load documents:\n",
      "\n",
      "```python\n",
      "from langchain_community.document_loaders import WebBaseLoader\n",
      "\n",
      "loader = WebBaseLoader(\"https://www.example.com/\")\n",
      "```\n",
      "\n",
      "To bypass SSL verification errors during fetching, you can set the \"verify\" option:\n",
      "\n",
      "`loader.requests_kwargs = {'verify':False}`\n",
      "\n",
      "### Initialization with multiple pages\n",
      "\n",
      "You can also pass in a list of pages to load from.\n",
      "\n",
      "```python\n",
      "loader_multiple_pages = WebBaseLoader(\n",
      "    [\"https://www.example.com/\", \"https://google.com\"]\n",
      ")\n",
      "```\n",
      "\n",
      "## Load\n",
      "\n",
      "```python\n",
      "docs = loader.load()\n",
      "\n",
      "docs[0]\n",
      "```\n",
      "----------------------------------------------------------------------------------------------------\n",
      "```python\n",
      "loader_multiple_pages = WebBaseLoader(\n",
      "    [\"https://www.example.com/\", \"https://google.com\"]\n",
      ")\n",
      "```\n",
      "\n",
      "## Load\n",
      "\n",
      "```python\n",
      "docs = loader.load()\n",
      "\n",
      "docs[0]\n",
      "```\n",
      "\n",
      "```output\n",
      "Document(metadata={'source': 'https://www.example.com/', 'title': 'Example Domain', 'language': 'No language found.'}, page_content='\\n\\n\\nExample Domain\\n\\n\\n\\n\\n\\n\\n\\nExample Domain\\nThis domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.\\nMore information...\\n\\n\\n\\n')\n",
      "```\n",
      "\n",
      "```python\n",
      "print(docs[0].metadata)\n",
      "```\n",
      "\n",
      "```output\n",
      "{'source': 'https://www.example.com/', 'title': 'Example Domain', 'language': 'No language found.'}\n",
      "```\n",
      "\n",
      "### Load multiple urls concurrently\n",
      "\n",
      "You can speed up the scraping process by scraping and parsing multiple urls concurrently.\n",
      "\n",
      "There are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren't concerned about being a good citizen, or you control the server you are scraping and don't care about load, you can change the `requests_per_second` parameter to increase the max concurrent requests.  Note, while this will speed up the scraping process, but may cause the server to block you.  Be careful!\n",
      "\n",
      "```python\n",
      "pip install -qU  nest_asyncio\n",
      "\n",
      "# fixes a bug with asyncio and jupyter\n",
      "import nest_asyncio\n",
      "\n",
      "nest_asyncio.apply()\n",
      "```\n",
      "\n",
      "```output\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "```\n",
      "\n",
      "```python\n",
      "loader = WebBaseLoader([\"https://www.example.com/\", \"https://google.com\"])\n",
      "loader.requests_per_second = 1\n",
      "docs = loader.aload()\n",
      "docs\n",
      "```\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Extends from the `WebBaseLoader`, `SitemapLoader` loads a sitemap from a given URL, and then scrapes and loads all pages in the sitemap, returning each page as a Document.\n",
      "\n",
      "The scraping is done concurrently. There are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren't concerned about being a good citizen, or you control the scrapped server, or don't care about load you can increase this limit. Note, while this will speed up the scraping process, it may cause the server to block you. Be careful!\n",
      "\n",
      "## Overview\n",
      "\n",
      "### Integration details\n",
      "\n",
      "| Class | Package | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/sitemap/)|\n",
      "| :--- | :--- | :---: | :---: |  :---: |\n",
      "| [SiteMapLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.sitemap.SitemapLoader.html#langchain_community.document_loaders.sitemap.SitemapLoader) | [langchain-community](https://python.langchain.com/api_reference/community/index.html) | ✅ | ❌ | ✅ |\n",
      "\n",
      "### Loader features\n",
      "\n",
      "| Source | Document Lazy Loading | Native Async Support\n",
      "| :---: | :---: | :---: |\n",
      "| SiteMapLoader | ✅ | ❌ |\n",
      "\n",
      "## Setup\n",
      "\n",
      "To access SiteMap document loader you'll need to install the `langchain-community` integration package.\n",
      "\n",
      "### Credentials\n",
      "\n",
      "No credentials are needed to run this.\n",
      "\n",
      "To enable automated tracing of your model calls, set your [LangSmith](https://docs.smith.langchain.com/) API key:\n",
      "\n",
      "```python\n",
      "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
      "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
      "```\n",
      "\n",
      "### Installation\n",
      "\n",
      "Install **langchain-community**.\n",
      "\n",
      "```python\n",
      "pip install -qU langchain-community\n",
      "```\n",
      "\n",
      "### Fix notebook asyncio bug\n",
      "\n",
      "```python\n",
      "import nest_asyncio\n",
      "\n",
      "nest_asyncio.apply()\n",
      "```\n",
      "\n",
      "## Initialization\n",
      "\n",
      "Now we can instantiate our model object and load documents:\n",
      "\n",
      "```python\n",
      "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
      "```\n",
      "----------------------------------------------------------------------------------------------------\n",
      "docs = loader.load()\n",
      "print(docs[0].page_content[:61])\n",
      "```\n",
      "\n",
      "### Loader Options\n",
      "\n",
      "- `urls` Required. A list of URLs to fetch.\n",
      "- `text_content` Retrieve only text content. Default is `False`.\n",
      "- `api_key` Browserbase API key. Default is `BROWSERBASE_API_KEY` env variable.\n",
      "- `project_id` Browserbase Project ID. Default is `BROWSERBASE_PROJECT_ID` env variable.\n",
      "- `session_id` Optional. Provide an existing Session ID.\n",
      "- `proxy` Optional. Enable/Disable Proxies.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "```python\n",
      "from langchain_nimble import NimbleSearchRetriever\n",
      "\n",
      "retriever = NimbleSearchRetriever(k=3)\n",
      "```\n",
      "\n",
      "## Usage\n",
      "\n",
      "`NimbleSearchRetriever` has these arguments:\n",
      "\n",
      "* `k` (optional) integer - Number of results to return (less than or equal to 20)\n",
      "* `api_key` (optional) string - Nimble's API key, can be sent directly when instantiating the retriever or with the environment variable (`NIMBLE_API_KEY`)\n",
      "* `search_engine` (optional) string - The search engine your query will be executed through, you can choose from\n",
      "  * `google_search` (default value) - Google's search engine\n",
      "  * `bing_search` - Bing's search engine\n",
      "  * `yandex_search` - Yandex search engine\n",
      "* `render` (optional) boolean - Enables or disables Javascript rendering on the target page (if enabled the results might return more slowly)\n",
      "* `locale` (optional) string - LCID standard locale used for the URL request. Alternatively, user can use auto for automatic locale based on country targeting.\n",
      "* `country` (optional) string - Country used to access the target URL, use ISO Alpha-2 Country Codes i.e. US, DE, GB\n",
      "* `parsing_type` (optional) string - The text structure of the returned `page_content`\n",
      "  * `plain_text` (default value) - Extracts just the text from the html\n",
      "  * `markdown` - Markdown format\n",
      "  * `simplified_html` - Compressed version of the original html document (~8% of the orignial html size)\n",
      "* `links` (optional) Array of strings - Array of links to the requested websites to scrape, if chosen will return the raw html content from these html **(THIS WILL ACTIVATE THE SECOND MODE)**\n",
      "\n",
      "You can read more about each argument in [Nimble's docs](https://docs.nimbleway.com/nimble-sdk/web-api/vertical-endpoints/serp-api/real-time-search-request#request-options).\n",
      "\n",
      "### Example of search & retrieve mode with a search query string\n",
      "\n",
      "**Fetching a single document will result in the following:**\n",
      "\n",
      "```python\n",
      "import json\n",
      "----------------------------------------------------------------------------------------------------\n",
      "const questionAnsweringPrompt = ChatPromptTemplate.fromMessages([\n",
      "  [\n",
      "    \"system\",\n",
      "    \"Answer the user's questions based on the below context:\\n\\n{context}\",\n",
      "  ],\n",
      "  [\"human\", \"{input}\"],\n",
      "]);\n",
      "\n",
      "const combineDocsChain = await createStuffDocumentsChain({\n",
      "  llm,\n",
      "  prompt: questionAnsweringPrompt,\n",
      "});\n",
      "\n",
      "const chain = await createRetrievalChain({\n",
      "  retriever: vectorStore.asRetriever(),\n",
      "  combineDocsChain,\n",
      "});\n",
      "\n",
      "const res = await chain.invoke({\n",
      "  input: question,\n",
      "});\n",
      "\n",
      "console.log(res.answer);\n",
      "```\n",
      "\n",
      "In this example, the `SerpAPILoader` is used to load web search results, which are then stored in memory using `MemoryVectorStore`. A retrieval chain is then used to retrieve the most relevant documents from the memory and answer the question based on these documents. This demonstrates how the `SerpAPILoader` can streamline the process of loading and processing web search results.\n"
     ]
    }
   ],
   "source": [
    "print_retrieved_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9e468ee-c92a-41f1-810c-d2f1dd13eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"Who won the India versus South Africa one day internation cricket match on 3rd dec 2025?\"\n",
    "docs2 = retriever.invoke(query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80fb847b-38cb-4c13-afee-01d560810e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "\\n\\n|\\n| \\n*   v\\n*   t\\n*   e\\nArgentina squad – 2021 Copa América winners (15th title)\\n|\\n| --- |\\n| \\n\\n1\\xa0Armani\\n2\\xa0Martínez Quarta\\n3\\xa0Tagliafico\\n4\\xa0Montiel\\n5\\xa0Paredes\\n6\\xa0Pezzella\\n7\\xa0De Paul\\n8\\xa0Acuña\\n9\\xa0Agüero\\n10\\xa0Messi\\xa0(c)\\n11\\xa0Di María\\n12\\xa0Marchesín\\n13\\xa0Romero\\n14\\xa0Palacios\\n15\\xa0González\\n16\\xa0J. Correa\\n17\\xa0Domínguez\\n18\\xa0Rodríguez\\n19\\xa0Otamendi\\n20\\xa0Lo Celso\\n21\\xa0Á. Correa\\n22\\xa0La. Martínez\\n23\\xa0E. Martínez\\n24\\xa0Gómez\\n25\\xa0Li. Martínez\\n26\\xa0Molina\\n27\\xa0Alvarez\\n28\\xa0Musso\\nCoach:\\xa0Scaloni\\n\\n| \\n\\n|\\n| \\n*   v\\n*   t\\n*   e\\nArgentina squad – 2022 FIFA World Cup winners (3rd title)\\n|\\n| --- |\\n| \\n\\n1\\xa0Armani\\n2\\xa0Foyth\\n3\\xa0Tagliafico\\n4\\xa0Montiel\\n5\\xa0Paredes\\n6\\xa0Pezzella\\n7\\xa0De Paul\\n8\\xa0Acuña\\n9\\xa0Alvarez\\n10\\xa0Messi\\xa0(c)\\n11\\xa0Di María\\n12\\xa0Rulli\\n13\\xa0Romero\\n14\\xa0Palacios\\n15\\xa0Correa\\n16\\xa0Almada\\n17\\xa0Gómez\\n18\\xa0Rodríguez\\n19\\xa0Otamendi\\n20\\xa0Mac Allister\\n21\\xa0Dybala\\n22\\xa0La. Martínez\\n23\\xa0E. Martínez\\n24\\xa0Fernández\\n25\\xa0Li. Martínez\\n26\\xa0Molina\\nCoach:\\xa0Scaloni\\n\\n| \\n\\n|\\n| \\n*   v\\n*   t\\n*   e\\nArgentina squad – 2024 Copa América winners (16th title)\\n|\\n| --- |\\n| \\n\\n1\\xa0Armani\\n2\\xa0Martínez Quarta\\n3\\xa0Tagliafico\\n4\\xa0Montiel\\n5\\xa0Paredes\\n6\\xa0Pezzella\\n7\\xa0De Paul\\n8\\xa0Acuña\\n9\\xa0Alvarez\\n10\\xa0Messi\\xa0(c)\\n11\\xa0Di María\\n12\\xa0Rulli\\n13\\xa0Romero\\n14\\xa0Palacios\\n15\\xa0González\\n16\\xa0Lo Celso\\n17\\xa0Garnacho\\n18\\xa0Rodríguez\\n19\\xa0Otamendi\\n20\\xa0Mac Allister\\n21\\xa0Carboni\\n22\\xa0La. Martínez\\n23\\xa0E. Martínez\\n24\\xa0Fernández\\n25\\xa0Li. Martínez\\n26\\xa0Molina\\nCoach:\\xa0Scaloni\\n\\n| \\n\\n|\\n|\\n| \\nAwards\\n|\\n| --- |\\n| \\n| \\n*   v\\n*   t\\n*   e\\n2014 FIFA World Cup Team of the Tournament\\n|\\n| --- |\\n| Statistical Team | \\nGoalkeeper Manuel NeuerDefenders Marcos Rojo Mats Hummels Thiago Silva Stefan de VrijMidfielders Oscar James Rodríguez Toni Kroos Philipp LahmForwards Thomas Müller Arjen Robben\\n|\\n| Fans\\' Team | \\nGoalkeeper Manuel NeuerDefenders David\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{\"query\": \"Euro 2024 host nation\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"UEFA Euro 2024 - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/UEFA_Euro_2024\", \"content\": \"Tournament details Host country Germany Dates 14 June – 14 July Teams 24 Venue(s) 10 (in 10 host cities) Final positions Champions Spain (4th title) Runners-up England Tournament statistics Matches played 51 Goals scored 117 (2.29 per match) Attendance 2,681,288 (52,574 per match) Top scorer(s) Harry Kane Georges Mikautadze Jamal Musiala Cody Gakpo Ivan Schranz Dani Olmo (3 goals each) Best player(s) Rodri Best young player Lamine Yamal ← 2020 2028 → The 2024 UEFA European Football Championship, commonly referred to as UEFA Euro 2024 (stylised as UEFA EURO 2024) or simply Euro 2024, was the 17th UEFA European Championship, the quadrennial international football championship organised by UEFA for the European men's national teams of their member associations. Germany hosted the tournament, which took place from 14 June to 14 July 2024. The tournament involved 24 teams, with Georgia making their European Championship debut. [4] Host nation Germany were eliminated by Spain in the quarter-finals; Spain went on to win the tournament for a record fourth time after defeating England 2–1 in the final.\", \"score\": 0.9104262, \"raw_content\": null}, {\"title\": \"UEFA Euro 2024 - Simple English Wikipedia, the free encyclopedia\", \"url\": \"https://simple.wikipedia.org/wiki/UEFA_Euro_2024\", \"content\": \"The 2024 UEFA European Football Championship, also known as UEFA Euro 2024 or simply Euro 2024, was the 17th edition of the UEFA European Championship. Germany was hosting the tournament. ... The UEFA Executive Committee voted for the host in a secret ballot, with only a simple majority (more than half of the valid votes) required to determine\", \"score\": 0.81418616, \"raw_content\": null}, {\"title\": \"Championnat d'Europe de football 2024 — Wikipédia\", \"url\":\n",
      "----------------------------------------------------------------------------------------------------\n",
      "118.07\\nWins: 69', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 24}), Document(page_content='MLB Team: Red Sox\\nPayroll in millions: 173.18\\nWins: 69', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 25}), Document(page_content='MLB Team: Indians\\nPayroll in millions: 78.43\\nWins: 68', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 26}), Document(page_content='MLB Team: Twins\\nPayroll in millions: 94.08\\nWins: 66', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 27}), Document(page_content='MLB Team: Rockies\\nPayroll in millions: 78.06\\nWins: 64', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 28}), Document(page_content='MLB Team: Cubs\\nPayroll in millions: 88.19\\nWins: 61', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 29}), Document(page_content='MLB Team: Astros\\nPayroll in millions: 60.65\\nWins: 55', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 30})]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "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\n",
      "----------------------------------------------------------------------------------------------------\n",
      "48888888888888888888888888888888888ow8AEUY0AYIoA888888888888888888888888888888888888B99998888888888888888888888888888q99999p88888cMcM8MM8s888888888888888888888888888888888888888888888888o8888888888888888888888888888888888s8c8ss88M8888888888888888888888888888888888888888B99998888888888888888888888888888q99999p8888888888888888888888888888888888888888888888888888888888888w0888888888888888888888888888888888888888888848w4w8848w0w8888888888888888888888888888B99998888888888888888888888888888q99999p888884ww0w000844w400ww08oggc888800088ww484888oAc8888w8wIQU88oAQ88888888888888888888888888884IY8888480w48w0c8oAAU8oAAU888skMc88s8ww4804w084MA888888B99998888888888888888888888888888q99999p88888E8Q8Iko0IsMogk8004woAA8888og8gAUMssw448o8wU888oUsQoAU88oAU088888888888888888888888888804Q8888cUkw840Q08sAAU8oAAU8888U88888AQwkkcAIQ04sI888888B99994008088888888888888888888888q99999p88888888888888888888888csMM8888s8s88888sc8888M888888888c8c88c8s8888888888888888888888888888Mc88888888s8888888Mc888MMc8sMMMMM88sc8888888888M8888888B9999scoIoc8888888888888888888888q99999p88888wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww088wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww888wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww08888B99998888888888888888888888888888q99999p88888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888B99998888888888888888888888888888q99999p88888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888B99998888888888888888888888888888q99999p88888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888B999988MYw88wUk848888888888888888q99999p88888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888B999988wQQ84IwU808888888888888888q99999p8888888888888888888888888888888888888888888888888888888888888888\n",
      "----------------------------------------------------------------------------------------------------\n",
      "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\n",
      "----------------------------------------------------------------------------------------------------\n",
      "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\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sim Racing World Championship 2022\",\"description\":\"World Championship 2022 Bahrain International Circuit Bahrain International Circuit Race Distance: 29 laps Track Length: 5.412 km\",\"url\":\"https://f1esports.com/world/results/2022\"},{\"title\":\"Our personal F1 2022 Predictions Championship : r/formula1\",\"description\":\"May 5, 2022 ... F1 2025 Driver Predictions from mathematical model. Leclerc and Sainz predicted to beat Hamilton and Albon. r/formula1 - F1 2025 Driver ...\",\"url\":\"https://www.reddit.com/r/formula1/comments/uitbbu/our_personal_f1_2022_predictions_championship/\"},{\"title\":\"Haas F1 Team Esports announces roster for 2022 F1 Esports Series ...\",\"description\":\"Sep 13, 2022 ... Haas F1 Team is ready to commence battle in the 2022 F1 Esports Series Pro Championship featuring an updated line-up for this season.\",\"url\":\"https://www.haasf1team.com/news/haas-f1-team-esports-announces-roster-2022-f1-esports-series-pro-championship\"},{\"title\":\"2022 F1 Deconstructors Championship : r/formula1\",\"description\":\"Nov 22, 2022 ... Congratulations to our 2022 champion Carlos Sainz. Alonso put in a late surge but had to settle for second place. Alfa put in the best effort ...\",\"url\":\"https://www.reddit.com/r/formula1/comments/z1pkkx/2022_f1_deconstructors_championship/\"}]\n"
     ]
    }
   ],
   "source": [
    "print_retrieved_docs(docs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8277f917-37bc-4b14-a961-c3f79f46fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = get_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fa929b-87ae-409a-bae2-3709b10974c7",
   "metadata": {},
   "source": [
    "## ROUTER Implementation using llm guided by the below prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c0ea33b-8614-4ecb-add6-993f6c46a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------- Router Prompt ----------------------------------------------------------\n",
    "router_instructions = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "\n",
    "The vectorstore contains documentation for LangChain and LangGraph products.\n",
    "                                    \n",
    "Use the vectorstore for questions on these topics. \n",
    "\n",
    "For questions where you know the answer, you can directly answer.\n",
    "\n",
    "For all else where you may not have the accurate answer, such as current events or time dependent information use web-search.\n",
    "\n",
    "Return JSON with single key, datasource, that is 'websearch' or 'vectorstore' depending on the question.\n",
    "\n",
    "Enclose your JSON output within the code fence: ```json and ```\n",
    "\n",
    "Do not provide any other information\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c27c4bc-0241-4604-89e0-cb3577a2672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"Explain how to create an agentic application using LangChain\"\n",
    "query2 = \"What are the top headlines of today?\"\n",
    "query3 = \"What is the weather in Bangalore right now?\"\n",
    "query4 = \"What are the types of agent memory?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea861c77-b6dc-4ee1-8ca8-1e5ee4977814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"datasource\": \"vectorstore\"\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "  \"datasource\": \"websearch\"\n",
      "}\n",
      "```\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"datasource\": \"websearch\"\n",
      "}\n",
      "```\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"datasource\": \"vectorstore\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Test router\n",
    "results1 = llm.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=query1)])\n",
    "print(results1.content)\n",
    "\n",
    "results2 = llm.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=query2)])\n",
    "print(results2.content)\n",
    "\n",
    "results3 = llm.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=query3)])\n",
    "print(results3.content)\n",
    "\n",
    "results4 = llm.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=query4)])\n",
    "print(results4.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d7f48c-e344-4cb9-81f1-3c494a0fe833",
   "metadata": {},
   "source": [
    "## Prompt to implement the Grader for verifying context relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89b300b9-27dd-4d71-a1fa-2b44422df40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval Grader \n",
    "\n",
    "# Doc grader instructions \n",
    "doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
    "\n",
    "This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "\n",
    "Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499906b-2c3d-408b-8108-c4e06ad29182",
   "metadata": {},
   "source": [
    "## Test the Grader for context relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4456b6fd-52c3-4d16-a861-12a572b2fb53",
   "metadata": {},
   "source": [
    "### 1. Positive test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a892ec0-0bea-4cd9-be04-0ced307815d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Explain how to create an agentic application using LangChain\n",
      "```python\n",
      "\n",
      "```\n",
      "\n",
      "```javascript\n",
      "\n",
      "```\n",
      "\n",
      "LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and [more](/oss/integrations/providers/overview). LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n",
      "\n",
      "We recommend you use LangChain if you want to quickly build agents and autonomous applications. Use [LangGraph](/oss/langgraph/overview), our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n",
      "\n",
      "LangChain [agents](/oss/langchain/agents) are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n",
      "\n",
      "## \n",
      "```\n",
      "```javascript\n",
      "\n",
      "```\n",
      "\n",
      "##documentation, see here.\\\\n\\\\nGetting Started#\\\\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\\\\n\\\\nGetting Started Documentation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nModules#\\\\nThere are several main modules that LangChain provides support for.\\\\nFor each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides.\\\\nThese modules are, in increasing order of complexity:\\\\n\\\\nModels: The various model types and model integrations LangChain supports.\\\\nPrompts: This includes prompt management, prompt optimization, and prompt serialization.\\\\nMemory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\\\nIndexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\\\\nChains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\\\nAgents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse Cases#\\\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\\\n\\\\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\\\\nQuestion Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those**LangChain v1 is a focused, production-ready foundation for building agents.** We've streamlined the framework around three core improvements:\n",
      "\n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "\n",
      "To upgrade,\n",
      "\n",
      "\n",
      "\n",
      "For a complete list of changes, see the [migration guide](/oss/migrate/langchain-v1).\n",
      "\n",
      "## `create_agent`\n",
      "\n",
      "@[`create_agent`] is the standard way to build agents in LangChain 1.0. It provides a simpler interface than @[`langgraph.prebuilt.create_react_agent`][create_react_agent] while offering greater customization potential by using [middleware](#middleware).\n",
      "\n",
      "```python\n",
      "from langchain.agents import create_agent\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[search_web, analyze_data, send_email],\n",
      "    system_prompt=\"You are a helpful research assistant.\"\n",
      ")\n",
      "\n",
      "result = agent.invoke({\n",
      "    \"messages\": [\n",
      "        {\"role\": \"user\", \"content\": \"Research AI safety trends\"}\n",
      "    ]\n",
      "})\n",
      "```\n",
      "\n",
      "Under the hood, @[`create_agent`] is built on the basic agent loop -- calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:\n",
      "\n",
      "\n",
      "\n",
      "For more information, see [Agents](/oss/langchain/agents).\n",
      "\n",
      "### Middleware\n",
      "\n",
      "Middleware is the defining feature of @[`create_agent`]. It offers a highly customizable entry-point, raising the ceiling for what you can build.\n",
      "\n",
      "Great agents require [context engineering](/oss/langchain/context-engineering): getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.\n",
      "\n",
      "#### Prebuilt middleware\n",
      "\n",
      "LangChain provides a few [prebuilt middlewares](/oss/langchain/middleware#built-in-middleware) for common patterns, including:\n",
      "\n",
      "- @[`PIIMiddleware`]: Redact sensitive information before sending to the model\n",
      "- @[`SummarizationMiddleware`]: Condense conversation history when it gets too long\n",
      "- @[`HumanInTheLoopMiddleware`]: Require approval for sensitive tool calls'\\n' +\n",
      "    'Build\\n' +\n",
      "    '\\n' +\n",
      "    'LangChain is a framework to build with LLMs by chaining interoperable components. LangGraph is the framework for building\\n' +\n",
      "    'controllable agentic workflows.\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    'Run\\n' +\n",
      "    '\\n' +\n",
      "    'Deploy your LLM applications at scale with LangGraph Cloud, our infrastructure purpose-built for agents.\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    'Manage\\n' +\n",
      "    '\\n' +\n",
      "    \"Debug, collaborate, test, and monitor your LLM app in LangSmith - whether it's built with a LangChain framework or not. \\n\" +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    'BUILD YOUR APP WITH LANGCHAIN\\n' +\n",
      "    '\\n' +\n",
      "    'Build context-aware, reasoning applications with LangChain’s flexible framework that leverages your company’s data and APIs.\\n' +\n",
      "    'Future-proof your application by making vendor optionality part of your LLM infrastructure design.\\n' +\n",
      "    '\\n' +\n",
      "    'Learn more about LangChain\\n' +\n",
      "    '\\n' +\n",
      "    '[/langchain]\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    'RUN AT SCALE WITH LANGGRAPH CLOUD\\n' +\n",
      "    '\\n' +\n",
      "    'Deploy your LangGraph app with LangGraph Cloud for fault-tolerant scalability - including support for async background jobs,\\n' +\n",
      "    'built-in persistence, and distributed task queues.\\n' +\n",
      "    '\\n' +\n",
      "    'Learn more about LangGraph\\n' +\n",
      "    '\\n' +\n",
      "    '[/langgraph]\\n' +\n",
      "    '[https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667c6d7284e58f4743a430e6_Langgraph%20UI-home-2.webp]\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    'MANAGE LLM PERFORMANCE WITH LANGSMITH\\n' +\n",
      "    '\\n' +\n",
      "    'Ship faster with LangSmith’s debug, test, deploy, and monitoring workflows. Don’t rely on “vibes” – add engineering rigor to your\\n' +\n",
      "    'LLM-development workflow, whether you’re building with LangChain or not.\\n' +\n",
      "    '\\n' +\n",
      "    'Learn more about LangSmith\\n' +\n",
      "    '\\n' +\n",
      "    '[/langsmith]\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    'HEAR FROM OUR HAPPY CUSTOMERS\\n' +\n",
      "    '\\n' +**LangChain v1 is a focused, production-ready foundation for building agents.** We've streamlined the framework around three core improvements:\n",
      "\n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "\n",
      "To upgrade,\n",
      "\n",
      "\n",
      "\n",
      "For a complete list of changes, see the [migration guide](/oss/migrate/langchain-v1).\n",
      "\n",
      "## `createAgent`\n",
      "\n",
      "`createAgent` is the standard way to build agents in LangChain 1.0. It provides a simpler interface than the prebuilt `createReactAgent` exported from LangGraph while offering greater customization potential by using middleware.\n",
      "\n",
      "```ts\n",
      "import { createAgent } from \"langchain\";\n",
      "\n",
      "const agent = createAgent({\n",
      "  model: \"claude-sonnet-4-5-20250929\",\n",
      "  tools: [getWeather],\n",
      "  systemPrompt: \"You are a helpful assistant.\",\n",
      "});\n",
      "\n",
      "const result = await agent.invoke({\n",
      "  messages: [\n",
      "    { role: \"user\", content: \"What is the weather in Tokyo?\" },\n",
      "  ],\n",
      "});\n",
      "\n",
      "console.log(result.content);\n",
      "```\n",
      "\n",
      "Under the hood, `createAgent` is built on the basic agent loop -- calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:\n",
      "\n",
      "\n",
      "\n",
      "For more information, see [Agents](/oss/langchain/agents).\n",
      "\n",
      "### Middleware\n",
      "\n",
      "Middleware is the defining feature of `createAgent`. It makes `createAgent` highly customizable, raising the ceiling for what you can build.\n",
      "\n",
      "Great agents require [context engineering](/oss/langchain/context-engineering): getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.\n",
      "\n",
      "#### Prebuilt middleware\n",
      "\n",
      "LangChain provides a few [prebuilt middlewares](/oss/langchain/middleware#built-in-middleware) for common patterns, including:\n",
      "\n",
      "- `summarizationMiddleware`: Condense conversation history when it gets too long\n",
      "- `humanInTheLoopMiddleware`: Require approval for sensitive tool calls\n",
      "- `piiRedactionMiddleware`: Redact sensitive information before sending to the modelIn the **Learn** section of the documentation, you'll find a collection of tutorials, conceptual overviews, and additional resources to help you build powerful applications with LangChain and LangGraph.\n",
      "\n",
      "## Use Cases\n",
      "\n",
      "Below are tutorials for common use cases, organized by framework.\n",
      "\n",
      "### LangChain\n",
      "\n",
      "[LangChain](/oss/langchain/overview) [agent](/oss/langchain/agents) implementations make it easy to get started for most use cases.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "```\n",
      "\n",
      "### LangGraph\n",
      "\n",
      "LangChain's [agent](/oss/langchain/agents) implementations use [LangGraph](/oss/langgraph/overview) primitives.\n",
      "If deeper customization is required, agents can be implemented directly in LangGraph.\n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "```\n",
      "\n",
      "## Conceptual Overviews\n",
      "\n",
      "These guides explain the core concepts and APIs underlying LangChain and LangGraph.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## Additional Resources[Document(page_content=\"LangChain is a framework for developing applications powered by large language models (LLMs).\\nLangChain simplifies every stage of the LLM application lifecycle:\\nDevelopment: Build your applications using LangChain's open-source building blocks and components. Hit the ground running using third-party integrations and Templates.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your chains, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn any chain into an API with LangServe.\\nlangchain-core: Base abstractions and LangChain Expression Language.\\nlangchain-community: Third party integrations.\\nPartner packages (e.g. langchain-openai, langchain-anthropic, etc.): Some integrations have been further split into their own lightweight packages that only depend on langchain-core.\\nlangchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\\nlanggraph: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\\nlangserve: Deploy LangChain chains as REST APIs.\\nThe broader ecosystem includes:\\nLangSmith: A developer platform that lets you debug, test, evaluate, and monitor LLM applications and seamlessly integrates with LangChain.\\nGet started\\nWe recommend following our Quickstart guide to familiarize yourself with the framework by building your first LangChain application.\\nSee here for instructions on how to install LangChain, set up your environment, and start building.\\nnote\\nThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.\\nUse cases\\nIf you're looking to build something specific or are more of a hands-on learner, check out our use-cases. They're walkthroughs and techniques for common end-to-end tasks, such as:\\nQuestion answering with RAG\\nExtracting structured output\\nChatbots\\nand more!\\nExpression Language\\nLangChain Expression Language (LCEL) is the foundation of many of LangChain's```python\n",
      "\n",
      "```\n",
      "\n",
      "```javascript\n",
      "\n",
      "```\n",
      "\n",
      "Trusted by companies shaping the future of agents-- including Klarna, Replit, Elastic, and more-- LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\n",
      "\n",
      "LangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/langchain/models) and [tools](/oss/langchain/tools).\n",
      "\n",
      "We will commonly use [LangChain](/oss/langchain/overview) components throughout the documentation to integrate models and tools, but you don't need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain's [agents](/oss/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops.\n",
      "\n",
      "LangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\n",
      "\n",
      "## \n",
      "```\n",
      "\n",
      "```javascript\n",
      "\n",
      "```\n",
      "\n",
      "Then, create a simple hello world example:\n",
      "\n",
      "```python\n",
      "```python\n",
      "from langgraph.graph import StateGraph, MessagesState, START, END\n",
      "\n",
      "def mock_llm(state: MessagesState):\n",
      "    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\n",
      "\n",
      "graph = StateGraph(MessagesState)\n",
      "graph.add_node(mock_llm)\n",
      "graph.add_edge(START, \"mock_llm\")\n",
      "graph.add_edge(\"mock_llm\", END)\n",
      "graph = graph.compile()\n",
      "\n",
      "graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\n",
      "```\n",
      "```\n",
      "\n",
      "```javascript\n",
      "```typescript\n",
      "import { MessagesAnnotation, StateGraph, START, END } from \"@langchain/langgraph\";\n",
      "\n",
      "const mockLlm = (state: typeof MessagesAnnotation.State) => {\n",
      "  return { messages: [{ role: \"ai\", content: \"hello world\" }] };\n",
      "};\n",
      "\n",
      "const graph = new StateGraph(MessagesAnnotation)\n",
      "  .addNode(\"mock_llm\", mockLlm)\n",
      "  .addEdge(START, \"mock_llm\")\n",
      "  .addEdge(\"mock_llm\", END)\n",
      "  .compile();\n",
      "Result:  ```json\n",
      "{\n",
      "  \"binary_score\": \"yes\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "question = query1\n",
    "print(\"Question: \", question)\n",
    "\n",
    "# Use the vector database to retrieve the top k chunks for the given question\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "txt = \"\"\n",
    "for d in docs:\n",
    "    txt += d.page_content  # \n",
    "\n",
    "print(txt)\n",
    "\n",
    "doc_grader_prompt_formatted = doc_grader_prompt.format(document=txt, question=question)\n",
    "result = llm.invoke([SystemMessage(content=doc_grader_instructions)] + [HumanMessage(content=doc_grader_prompt_formatted)])\n",
    "\n",
    "print(\"Result: \", result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdb6795-54eb-42c8-9f14-29251504f84c",
   "metadata": {},
   "source": [
    "### 2. Negative Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8a47bfc-20d1-4cf4-a85c-402c7547c0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the top headlines of today?\n",
      "```python\n",
      "\n",
      "```\n",
      "\n",
      "```javascript\n",
      "\n",
      "```\n",
      "\n",
      "LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and [more](/oss/integrations/providers/overview). LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n",
      "\n",
      "We recommend you use LangChain if you want to quickly build agents and autonomous applications. Use [LangGraph](/oss/langgraph/overview), our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n",
      "\n",
      "LangChain [agents](/oss/langchain/agents) are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n",
      "\n",
      "## \n",
      "```\n",
      "```javascript\n",
      "\n",
      "```\n",
      "\n",
      "##documentation, see here.\\\\n\\\\nGetting Started#\\\\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\\\\n\\\\nGetting Started Documentation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nModules#\\\\nThere are several main modules that LangChain provides support for.\\\\nFor each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides.\\\\nThese modules are, in increasing order of complexity:\\\\n\\\\nModels: The various model types and model integrations LangChain supports.\\\\nPrompts: This includes prompt management, prompt optimization, and prompt serialization.\\\\nMemory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\\\nIndexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\\\\nChains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\\\nAgents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse Cases#\\\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\\\n\\\\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\\\\nQuestion Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those**LangChain v1 is a focused, production-ready foundation for building agents.** We've streamlined the framework around three core improvements:\n",
      "\n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "\n",
      "To upgrade,\n",
      "\n",
      "\n",
      "\n",
      "For a complete list of changes, see the [migration guide](/oss/migrate/langchain-v1).\n",
      "\n",
      "## `create_agent`\n",
      "\n",
      "@[`create_agent`] is the standard way to build agents in LangChain 1.0. It provides a simpler interface than @[`langgraph.prebuilt.create_react_agent`][create_react_agent] while offering greater customization potential by using [middleware](#middleware).\n",
      "\n",
      "```python\n",
      "from langchain.agents import create_agent\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[search_web, analyze_data, send_email],\n",
      "    system_prompt=\"You are a helpful research assistant.\"\n",
      ")\n",
      "\n",
      "result = agent.invoke({\n",
      "    \"messages\": [\n",
      "        {\"role\": \"user\", \"content\": \"Research AI safety trends\"}\n",
      "    ]\n",
      "})\n",
      "```\n",
      "\n",
      "Under the hood, @[`create_agent`] is built on the basic agent loop -- calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:\n",
      "\n",
      "\n",
      "\n",
      "For more information, see [Agents](/oss/langchain/agents).\n",
      "\n",
      "### Middleware\n",
      "\n",
      "Middleware is the defining feature of @[`create_agent`]. It offers a highly customizable entry-point, raising the ceiling for what you can build.\n",
      "\n",
      "Great agents require [context engineering](/oss/langchain/context-engineering): getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.\n",
      "\n",
      "#### Prebuilt middleware\n",
      "\n",
      "LangChain provides a few [prebuilt middlewares](/oss/langchain/middleware#built-in-middleware) for common patterns, including:\n",
      "\n",
      "- @[`PIIMiddleware`]: Redact sensitive information before sending to the model\n",
      "- @[`SummarizationMiddleware`]: Condense conversation history when it gets too long\n",
      "- @[`HumanInTheLoopMiddleware`]: Require approval for sensitive tool calls'\\n' +\n",
      "    'Build\\n' +\n",
      "    '\\n' +\n",
      "    'LangChain is a framework to build with LLMs by chaining interoperable components. LangGraph is the framework for building\\n' +\n",
      "    'controllable agentic workflows.\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    'Run\\n' +\n",
      "    '\\n' +\n",
      "    'Deploy your LLM applications at scale with LangGraph Cloud, our infrastructure purpose-built for agents.\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    'Manage\\n' +\n",
      "    '\\n' +\n",
      "    \"Debug, collaborate, test, and monitor your LLM app in LangSmith - whether it's built with a LangChain framework or not. \\n\" +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    'BUILD YOUR APP WITH LANGCHAIN\\n' +\n",
      "    '\\n' +\n",
      "    'Build context-aware, reasoning applications with LangChain’s flexible framework that leverages your company’s data and APIs.\\n' +\n",
      "    'Future-proof your application by making vendor optionality part of your LLM infrastructure design.\\n' +\n",
      "    '\\n' +\n",
      "    'Learn more about LangChain\\n' +\n",
      "    '\\n' +\n",
      "    '[/langchain]\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    'RUN AT SCALE WITH LANGGRAPH CLOUD\\n' +\n",
      "    '\\n' +\n",
      "    'Deploy your LangGraph app with LangGraph Cloud for fault-tolerant scalability - including support for async background jobs,\\n' +\n",
      "    'built-in persistence, and distributed task queues.\\n' +\n",
      "    '\\n' +\n",
      "    'Learn more about LangGraph\\n' +\n",
      "    '\\n' +\n",
      "    '[/langgraph]\\n' +\n",
      "    '[https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/667c6d7284e58f4743a430e6_Langgraph%20UI-home-2.webp]\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    'MANAGE LLM PERFORMANCE WITH LANGSMITH\\n' +\n",
      "    '\\n' +\n",
      "    'Ship faster with LangSmith’s debug, test, deploy, and monitoring workflows. Don’t rely on “vibes” – add engineering rigor to your\\n' +\n",
      "    'LLM-development workflow, whether you’re building with LangChain or not.\\n' +\n",
      "    '\\n' +\n",
      "    'Learn more about LangSmith\\n' +\n",
      "    '\\n' +\n",
      "    '[/langsmith]\\n' +\n",
      "    '\\n' +\n",
      "    '\\n' +\n",
      "    'HEAR FROM OUR HAPPY CUSTOMERS\\n' +\n",
      "    '\\n' +**LangChain v1 is a focused, production-ready foundation for building agents.** We've streamlined the framework around three core improvements:\n",
      "\n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "\n",
      "To upgrade,\n",
      "\n",
      "\n",
      "\n",
      "For a complete list of changes, see the [migration guide](/oss/migrate/langchain-v1).\n",
      "\n",
      "## `createAgent`\n",
      "\n",
      "`createAgent` is the standard way to build agents in LangChain 1.0. It provides a simpler interface than the prebuilt `createReactAgent` exported from LangGraph while offering greater customization potential by using middleware.\n",
      "\n",
      "```ts\n",
      "import { createAgent } from \"langchain\";\n",
      "\n",
      "const agent = createAgent({\n",
      "  model: \"claude-sonnet-4-5-20250929\",\n",
      "  tools: [getWeather],\n",
      "  systemPrompt: \"You are a helpful assistant.\",\n",
      "});\n",
      "\n",
      "const result = await agent.invoke({\n",
      "  messages: [\n",
      "    { role: \"user\", content: \"What is the weather in Tokyo?\" },\n",
      "  ],\n",
      "});\n",
      "\n",
      "console.log(result.content);\n",
      "```\n",
      "\n",
      "Under the hood, `createAgent` is built on the basic agent loop -- calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:\n",
      "\n",
      "\n",
      "\n",
      "For more information, see [Agents](/oss/langchain/agents).\n",
      "\n",
      "### Middleware\n",
      "\n",
      "Middleware is the defining feature of `createAgent`. It makes `createAgent` highly customizable, raising the ceiling for what you can build.\n",
      "\n",
      "Great agents require [context engineering](/oss/langchain/context-engineering): getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.\n",
      "\n",
      "#### Prebuilt middleware\n",
      "\n",
      "LangChain provides a few [prebuilt middlewares](/oss/langchain/middleware#built-in-middleware) for common patterns, including:\n",
      "\n",
      "- `summarizationMiddleware`: Condense conversation history when it gets too long\n",
      "- `humanInTheLoopMiddleware`: Require approval for sensitive tool calls\n",
      "- `piiRedactionMiddleware`: Redact sensitive information before sending to the modelIn the **Learn** section of the documentation, you'll find a collection of tutorials, conceptual overviews, and additional resources to help you build powerful applications with LangChain and LangGraph.\n",
      "\n",
      "## Use Cases\n",
      "\n",
      "Below are tutorials for common use cases, organized by framework.\n",
      "\n",
      "### LangChain\n",
      "\n",
      "[LangChain](/oss/langchain/overview) [agent](/oss/langchain/agents) implementations make it easy to get started for most use cases.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "```\n",
      "\n",
      "### LangGraph\n",
      "\n",
      "LangChain's [agent](/oss/langchain/agents) implementations use [LangGraph](/oss/langgraph/overview) primitives.\n",
      "If deeper customization is required, agents can be implemented directly in LangGraph.\n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "```\n",
      "\n",
      "## Conceptual Overviews\n",
      "\n",
      "These guides explain the core concepts and APIs underlying LangChain and LangGraph.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## Additional Resources[Document(page_content=\"LangChain is a framework for developing applications powered by large language models (LLMs).\\nLangChain simplifies every stage of the LLM application lifecycle:\\nDevelopment: Build your applications using LangChain's open-source building blocks and components. Hit the ground running using third-party integrations and Templates.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your chains, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn any chain into an API with LangServe.\\nlangchain-core: Base abstractions and LangChain Expression Language.\\nlangchain-community: Third party integrations.\\nPartner packages (e.g. langchain-openai, langchain-anthropic, etc.): Some integrations have been further split into their own lightweight packages that only depend on langchain-core.\\nlangchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\\nlanggraph: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\\nlangserve: Deploy LangChain chains as REST APIs.\\nThe broader ecosystem includes:\\nLangSmith: A developer platform that lets you debug, test, evaluate, and monitor LLM applications and seamlessly integrates with LangChain.\\nGet started\\nWe recommend following our Quickstart guide to familiarize yourself with the framework by building your first LangChain application.\\nSee here for instructions on how to install LangChain, set up your environment, and start building.\\nnote\\nThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.\\nUse cases\\nIf you're looking to build something specific or are more of a hands-on learner, check out our use-cases. They're walkthroughs and techniques for common end-to-end tasks, such as:\\nQuestion answering with RAG\\nExtracting structured output\\nChatbots\\nand more!\\nExpression Language\\nLangChain Expression Language (LCEL) is the foundation of many of LangChain's```python\n",
      "\n",
      "```\n",
      "\n",
      "```javascript\n",
      "\n",
      "```\n",
      "\n",
      "Trusted by companies shaping the future of agents-- including Klarna, Replit, Elastic, and more-- LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\n",
      "\n",
      "LangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/langchain/models) and [tools](/oss/langchain/tools).\n",
      "\n",
      "We will commonly use [LangChain](/oss/langchain/overview) components throughout the documentation to integrate models and tools, but you don't need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain's [agents](/oss/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops.\n",
      "\n",
      "LangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\n",
      "\n",
      "## \n",
      "```\n",
      "\n",
      "```javascript\n",
      "\n",
      "```\n",
      "\n",
      "Then, create a simple hello world example:\n",
      "\n",
      "```python\n",
      "```python\n",
      "from langgraph.graph import StateGraph, MessagesState, START, END\n",
      "\n",
      "def mock_llm(state: MessagesState):\n",
      "    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\n",
      "\n",
      "graph = StateGraph(MessagesState)\n",
      "graph.add_node(mock_llm)\n",
      "graph.add_edge(START, \"mock_llm\")\n",
      "graph.add_edge(\"mock_llm\", END)\n",
      "graph = graph.compile()\n",
      "\n",
      "graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\n",
      "```\n",
      "```\n",
      "\n",
      "```javascript\n",
      "```typescript\n",
      "import { MessagesAnnotation, StateGraph, START, END } from \"@langchain/langgraph\";\n",
      "\n",
      "const mockLlm = (state: typeof MessagesAnnotation.State) => {\n",
      "  return { messages: [{ role: \"ai\", content: \"hello world\" }] };\n",
      "};\n",
      "\n",
      "const graph = new StateGraph(MessagesAnnotation)\n",
      "  .addNode(\"mock_llm\", mockLlm)\n",
      "  .addEdge(START, \"mock_llm\")\n",
      "  .addEdge(\"mock_llm\", END)\n",
      "  .compile();\n",
      "Result:  ```json\n",
      "{\n",
      "  \"binary_score\": \"no\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "question = query2\n",
    "print(question)\n",
    "\n",
    "txt = \"\"\n",
    "for d in docs:\n",
    "    txt += d.page_content  # \n",
    "\n",
    "print(txt)\n",
    "\n",
    "doc_grader_prompt_formatted = doc_grader_prompt.format(document=txt, question=question)\n",
    "result = llm.invoke([SystemMessage(content=doc_grader_instructions)] + [HumanMessage(content=doc_grader_prompt_formatted)])\n",
    "\n",
    "print(\"Result: \", result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb7e56e-7116-4d95-9c0f-93fd4c38316a",
   "metadata": {},
   "source": [
    "## Prompt for Answer Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54483649-c8d0-448d-93c1-d14ca7d29306",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "\n",
    "# Prompt\n",
    "rag_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "\n",
    "Here is the context to use to answer the question:\n",
    "\n",
    "{context} \n",
    "\n",
    "Think carefully about the above context. \n",
    "\n",
    "Now, review the user question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Provide an answer to this questions using only the above context. \n",
    "\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    sources = []\n",
    "    txt = \"\"\n",
    "    for doc in docs:\n",
    "        txt += \"\\n\\n\" + doc.page_content\n",
    "        sources.append(doc.metadata[\"source\"])\n",
    "\n",
    "    print(f\"Sources = {sources}, num_sources = {len(sources)}, num_docs = {len(docs)}\")\n",
    "    \n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf061414-9c5c-4252-9ac8-882906cdc7fb",
   "metadata": {},
   "source": [
    "### 1. Positive Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b074caec-4aa6-4545-a380-dd28e45788fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sources = ['C:\\\\home\\\\ananth\\\\research\\\\packages\\\\docs\\\\src\\\\oss\\\\langchain\\\\overview.mdx', 'C:\\\\home\\\\ananth\\\\research\\\\packages\\\\docs\\\\src\\\\oss\\\\python\\\\integrations\\\\document_loaders\\\\docusaurus.mdx', 'C:\\\\home\\\\ananth\\\\research\\\\packages\\\\docs\\\\src\\\\oss\\\\python\\\\releases\\\\langchain-v1.mdx', 'C:\\\\home\\\\ananth\\\\research\\\\packages\\\\docs\\\\src\\\\oss\\\\javascript\\\\integrations\\\\document_loaders\\\\web_loaders\\\\recursive_url_loader.mdx', 'C:\\\\home\\\\ananth\\\\research\\\\packages\\\\docs\\\\src\\\\oss\\\\javascript\\\\releases\\\\langchain-v1.mdx', 'C:\\\\home\\\\ananth\\\\research\\\\packages\\\\docs\\\\src\\\\oss\\\\learn.mdx', 'C:\\\\home\\\\ananth\\\\research\\\\packages\\\\docs\\\\src\\\\oss\\\\python\\\\integrations\\\\document_loaders\\\\diffbot.mdx', 'C:\\\\home\\\\ananth\\\\research\\\\packages\\\\docs\\\\src\\\\oss\\\\langgraph\\\\overview.mdx'], num_sources = 8, num_docs = 8\n",
      "====================================================================================================\n",
      "LangChain offers a straightforward way to build agents with `create_agent`, which simplifies the process compared to LangGraph's more complex approach. This function utilizes middleware for customization, allowing control over dynamic prompts, summarization, and tool access. For basic agent usage, you don’t need to know LangGraph, as LangChain agents are built on top of it to provide features like durable execution and streaming.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test\n",
    "question = query1\n",
    "docs = retriever.invoke(question)\n",
    "docs_txt = format_docs(docs)\n",
    "rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "print(\"=\" * 100)\n",
    "print(generation.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74b528-4fc2-4524-b351-e16dfbe9d9d9",
   "metadata": {},
   "source": [
    "### 2. Negative Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d2fb3c4-314c-4a8c-9d0e-4e8706560de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sources = ['C:\\\\home\\\\ananth\\\\research\\\\packages\\\\docs\\\\src\\\\oss\\\\python\\\\integrations\\\\tools\\\\playwright.mdx', 'C:\\\\home\\\\ananth\\\\research\\\\packages\\\\docs\\\\src\\\\oss\\\\python\\\\integrations\\\\tools\\\\playwright.mdx', 'C:\\\\home\\\\ananth\\\\research\\\\packages\\\\docs\\\\src\\\\oss\\\\python\\\\integrations\\\\tools\\\\playwright.mdx', 'C:\\\\home\\\\ananth\\\\research\\\\packages\\\\docs\\\\src\\\\oss\\\\python\\\\integrations\\\\tools\\\\linkup_search.mdx', 'C:\\\\home\\\\ananth\\\\research\\\\packages\\\\docs\\\\src\\\\oss\\\\python\\\\integrations\\\\tools\\\\playwright.mdx', 'C:\\\\home\\\\ananth\\\\research\\\\packages\\\\docs\\\\src\\\\oss\\\\python\\\\integrations\\\\tools\\\\playwright.mdx', 'C:\\\\home\\\\ananth\\\\research\\\\packages\\\\docs\\\\src\\\\oss\\\\python\\\\integrations\\\\tools\\\\agentql.mdx', 'C:\\\\home\\\\ananth\\\\research\\\\packages\\\\docs\\\\src\\\\oss\\\\python\\\\integrations\\\\retrievers\\\\linkup_search.mdx'], num_sources = 8, num_docs = 8\n",
      "====================================================================================================\n",
      "Donald Trump has secured more than the 270 Electoral College votes needed to secure the presidency, according to NBC News. Top stories include China competition being a priority for Trump and his picks for various secretary positions. There are no new news alerts at this time.\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "question = query2\n",
    "docs = retriever.invoke(question)\n",
    "docs_txt = format_docs(docs)\n",
    "rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "print(\"=\" * 100)\n",
    "print(generation.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477a35d-1e6f-4fcd-8e47-c1f31687dd19",
   "metadata": {},
   "source": [
    "## Grader to test for Groundedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dbb5bbf2-cfc7-4007-ae89-75fb5ee32149",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hallucination Grader \n",
    "\n",
    "# Hallucination grader instructions \n",
    "hallucination_grader_instructions = \"\"\"\n",
    "\n",
    "You are a teacher grading a quiz. \n",
    "\n",
    "You will be given FACTS and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "\n",
    "(1) Ensure the STUDENT ANSWER is grounded in the FACTS. \n",
    "\n",
    "(2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
    "\n",
    "Score:\n",
    "\n",
    "A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
    "\n",
    "A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "hallucination_grader_prompt = \"\"\"FACTS: \\n\\n {documents} \\n\\n STUDENT ANSWER: {generation}. \n",
    "\n",
    "Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER is grounded in the FACTS. And a key, explanation, that contains an explanation of the score.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2606b945-2c72-4b93-ad84-07c5d9e371f0",
   "metadata": {},
   "source": [
    "### Test using documents and generation from above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0cbb1d65-8304-4a67-8ca3-355445e4845a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"binary_score\": \"yes\",\n",
      "  \"explanation\": \"The student answer accurately reflects information presented in the provided text. Specifically, it correctly states that Donald Trump secured over 270 electoral votes (as projected by NBC News), mentions China competition as a priority for Trump, and notes his picks for secretary positions – all of which are directly supported by the document. The inclusion of 'There are no new news alerts at this time' is also accurate based on the provided text. There is no hallucinated information.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(documents=docs_txt, generation=generation.content)\n",
    "result = llm.invoke([SystemMessage(content=hallucination_grader_instructions)] + [HumanMessage(content=hallucination_grader_prompt_formatted)])\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5c120b-34ee-4c66-9059-11f6e9bb10c3",
   "metadata": {},
   "source": [
    "## Answer Grader Prompt to test \"Answer Relevance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c1aceafa-f04e-469c-bce7-3cc92bbe913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Answer Grader \n",
    "\n",
    "# Answer grader instructions \n",
    "answer_grader_instructions = \"\"\"You are a teacher grading a quiz. \n",
    "\n",
    "You will be given a QUESTION and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "\n",
    "(1) The STUDENT ANSWER helps to answer the QUESTION\n",
    "\n",
    "Score:\n",
    "\n",
    "A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
    "\n",
    "The student can receive a score of yes if the answer contains extra information that is not explicitly asked for in the question.\n",
    "\n",
    "A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "answer_grader_prompt = \"\"\"QUESTION: \\n\\n {question} \\n\\n STUDENT ANSWER: {generation}. \n",
    "\n",
    "Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER meets the criteria. And a key, explanation, that contains an explanation of the score.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf2c3d-5415-4a7f-9bac-d315d90cf94f",
   "metadata": {},
   "source": [
    "### Test the Answer Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a87ee46-9281-4488-9036-8b33fecf423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down this student answer and determine its score based on the given question and grading criteria.\n",
      "\n",
      "**1. Understand the Question:**\n",
      "\n",
      "The question asks specifically about the vision models released *today* as part of Llama 3.2. It's a direct request for a list or identification of these models.\n",
      "\n",
      "**2. Analyze the Student Answer:**\n",
      "\n",
      "The student answer provides:\n",
      "\n",
      "*   Identification of two specific vision models: \"Llama 3.2 11B Vision Instruct\" and \"Llama 3.2 90B Vision Instruct.\"\n",
      "*   Information about their availability on Azure AI Model Catalog via managed compute.\n",
      "*   Contextual information regarding Meta's entry into multimodal AI, comparisons to other models (Anthropic Claude 3 Haiku, OpenAI GPT-4o mini), and the replacement of older Llama 3.1 models.\n",
      "\n",
      "**3. Evaluate Against Criteria:**\n",
      "\n",
      "The question asks for a list of vision models released as part of Llama 3.2. The student answer *does* provide this list (the two model names). While the answer includes additional information, the core request is fulfilled. Extra information isn't penalized; in fact, it can demonstrate broader understanding.\n",
      "\n",
      "**4. Conclusion:**\n",
      "\n",
      "The student’s response directly addresses the question by naming the vision models released as part of Llama 3.2. The extra details provide helpful context but don't detract from the core answer. Therefore, the answer meets all criteria.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"binary_score\": \"yes\",\n",
      "  \"explanation\": \"The student answer directly lists the two vision models released as part of Llama 3.2 (Llama 3.2 11B Vision Instruct and Llama 3.2 90B Vision Instruct), which is precisely what the question asked for. The additional context provided doesn't detract from this core response.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test \n",
    "question = \"What are the vision models released today as part of Llama 3.2?\"\n",
    "answer = \"The Llama 3.2 models released today include two vision models: Llama 3.2 11B Vision Instruct and Llama 3.2 90B Vision Instruct, which are available on Azure AI Model Catalog via managed compute. These models are part of Meta's first foray into multimodal AI and rival closed models like Anthropic's Claude 3 Haiku and OpenAI's GPT-4o mini in visual reasoning. They replace the older text-only Llama 3.1 models.\"\n",
    "\n",
    "# Test using question and generation from above \n",
    "answer_grader_prompt_formatted = answer_grader_prompt.format(question=question, generation=answer)\n",
    "result = llm.invoke([SystemMessage(content=answer_grader_instructions)] + [HumanMessage(content=answer_grader_prompt_formatted)])\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17941e9d-3e12-408a-897c-44241371440d",
   "metadata": {},
   "source": [
    "## Define the web search tool - replace the TAVILY_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24269f42-d7d9-4d4b-aa1d-4d9618f33a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ananth\\AppData\\Local\\Temp\\ipykernel_89016\\2349448989.py:6: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  web_search_tool = TavilySearchResults(k=3)\n"
     ]
    }
   ],
   "source": [
    "### Search\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba09a7da-79e2-4ac7-897e-5d3154793683",
   "metadata": {},
   "source": [
    "## Define the state for our RAG Agent app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "493c30c9-f04c-4fc0-a363-0a3e8ccca3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Graph state is a dictionary that contains information we want to propagate to, and modify in, each graph node.\n",
    "    \"\"\"\n",
    "    question : str # User question\n",
    "    generation : str # LLM generation\n",
    "    web_search : str # Binary decision to run web search\n",
    "    max_retries : int # Max number of retries for answer generation \n",
    "    answers : int # Number of answers generated\n",
    "    loop_step: Annotated[int, operator.add] \n",
    "    documents : List[str] # List of retrieved documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f0d5e-a12e-4349-8554-e06f88cda841",
   "metadata": {},
   "source": [
    "## Define all necessary functions that constitute nodes and booleans for conditional edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cf6ff056-df87-4169-9004-4165e4e0844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import END\n",
    "\n",
    "### Nodes\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Write retrieved documents to documents key in state\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    loop_step = state.get(\"loop_step\", 0)\n",
    "    \n",
    "    # RAG generation\n",
    "    docs_txt = format_docs(documents)\n",
    "    rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    # TBD JSON parsing\n",
    "    return {\"generation\": generation, \"loop_step\": loop_step+1}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\" \n",
    "    for d in documents:\n",
    "        doc_grader_prompt_formatted = doc_grader_prompt.format(document=d.page_content, question=question)\n",
    "        result = llm.invoke([SystemMessage(content=doc_grader_instructions)] + [HumanMessage(content=doc_grader_prompt_formatted)])\n",
    "        \n",
    "        # TBD check code fencing and JSON parsing\n",
    "        \n",
    "        grade = json.loads(result.content)['binary_score']\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"web_search\": web_search}\n",
    "    \n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "### Edges\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG \n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    route_question = llm.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=state[\"question\"])])\n",
    "\n",
    "    # TBD: JSON parsing - check for code fencing\n",
    "    source = json.loads(route_question.content)['datasource']\n",
    "    if source == 'websearch':\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    elif source == 'vectorstore':\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: NOT ALL DOCUMENTS ARE RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    max_retries = state.get(\"max_retries\", 3) # Default to 3 if not provided\n",
    "\n",
    "    hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(documents=format_docs(documents), generation=generation.content)\n",
    "    result = llm.invoke([SystemMessage(content=hallucination_grader_instructions)] + [HumanMessage(content=hallucination_grader_prompt_formatted)])\n",
    "\n",
    "    # TBD JSON parsing - code fencing\n",
    "    grade = json.loads(result.content)['binary_score']\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        # Test using question and generation from above \n",
    "        answer_grader_prompt_formatted = answer_grader_prompt.format(question=question, generation=generation.content)\n",
    "\n",
    "        # TBD JSON parsing - code fencing\n",
    "        result = llm.invoke([SystemMessage(content=answer_grader_instructions)] + [HumanMessage(content=answer_grader_prompt_formatted)])\n",
    "        grade = json.loads(result.content)['binary_score']\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        elif state[\"loop_step\"] <= max_retries:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: MAX RETRIES REACHED---\")\n",
    "            return \"max retries\"  \n",
    "    elif state[\"loop_step\"] <= max_retries:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "    else:\n",
    "        print(\"---DECISION: MAX RETRIES REACHED---\")\n",
    "        return \"max retries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb61f7d2-64bd-4e47-8257-dca81629c8ef",
   "metadata": {},
   "source": [
    "## Define the graph and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45ca217e-0302-4b15-b53b-14b8fd086f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search) # web search\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents) # grade documents\n",
    "workflow.add_node(\"generate\", generate) # generate\n",
    "\n",
    "# Build graph\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "        \"max retries\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7da400-55b0-4987-bb15-586505406fc9",
   "metadata": {},
   "source": [
    "## Execute the application - that is the compiled graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72f45f41-649b-4ce6-94a6-725709e21f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# inputs = {\"question\": \"What is Adobe's revenue for 2023?\", \"max_retries\": 3}\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# inputs = {\"question\": \"What are the new products developed Microsoft over 2021 to 2023 and how does it compare with Google during same period?\", \"max_retries\": 3}\u001b[39;00m\n\u001b[32m      3\u001b[39m inputs = {\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: query2, \u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m}\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\site-packages\\langgraph\\pregel\\main.py:2643\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2641\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2642\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2643\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2653\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:658\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    656\u001b[39m                 \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m    657\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m             \u001b[38;5;28minput\u001b[39m = \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\site-packages\\langgraph\\graph\\_branch.py:166\u001b[39m, in \u001b[36mBranchSpec._route\u001b[39m\u001b[34m(self, input, config, reader, writer)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    165\u001b[39m     value = \u001b[38;5;28minput\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._finish(writer, \u001b[38;5;28minput\u001b[39m, result, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:393\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    391\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m         ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    395\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mroute_question\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m---ROUTE QUESTION---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    114\u001b[39m route_question = llm.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=state[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m])])\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m source = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroute_question\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mdatasource\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source == \u001b[33m'\u001b[39m\u001b[33mwebsearch\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    117\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m---ROUTE QUESTION TO WEB SEARCH---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\json\\decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    333\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     end = _w(s, end).end()\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\json\\decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    353\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "During task with name '__start__' and id '5ec20042-1a84-2be3-6cc8-58dce8872a30'"
     ]
    }
   ],
   "source": [
    "# inputs = {\"question\": \"What is Adobe's revenue for 2023?\", \"max_retries\": 3}\n",
    "# inputs = {\"question\": \"What are the new products developed Microsoft over 2021 to 2023 and how does it compare with Google during same period?\", \"max_retries\": 3}\n",
    "inputs = {\"question\": query2, \"max_retries\": 3}\n",
    "\n",
    "for event in graph.stream(inputs, stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bf59159d-509c-410b-b410-fd8d7c410ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery1\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\site-packages\\langgraph\\pregel\\main.py:3068\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3066\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\site-packages\\langgraph\\pregel\\main.py:2643\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2641\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2642\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2643\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2653\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:658\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    656\u001b[39m                 \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m    657\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m             \u001b[38;5;28minput\u001b[39m = \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\site-packages\\langgraph\\graph\\_branch.py:166\u001b[39m, in \u001b[36mBranchSpec._route\u001b[39m\u001b[34m(self, input, config, reader, writer)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    165\u001b[39m     value = \u001b[38;5;28minput\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._finish(writer, \u001b[38;5;28minput\u001b[39m, result, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:393\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    391\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m         ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    395\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mroute_question\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m---ROUTE QUESTION---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    114\u001b[39m route_question = llm.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=state[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m])])\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m source = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroute_question\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mdatasource\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source == \u001b[33m'\u001b[39m\u001b[33mwebsearch\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    117\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m---ROUTE QUESTION TO WEB SEARCH---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\json\\decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    333\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     end = _w(s, end).end()\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\agentic_ai_dec2025_2026\\Lib\\json\\decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    353\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "During task with name '__start__' and id 'cd43a18a-2872-2831-969c-10d9cf31ee65'"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": query1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be347236-73aa-406f-a6d9-a6c2f60fc312",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c3400-15d6-4d96-83e5-32275086367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"generation\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3165dc-497f-482d-a81f-b5799c01adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e72c48-49c4-42bf-a4b8-ff4fd2244190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
